{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np # scientific computing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # reading images\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray # converting rgb images to grayscale\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'japaneseocr-1-8883b9dcab0a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = rgb2gray(mpimg.imread('images/test_image.png')[:,:,:3])\n",
    "img2 = rgb2gray(mpimg.imread('images/bird.png')[:,:,:3])\n",
    "img3 = rgb2gray(mpimg.imread('images/two_words.jpg')[:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeWithAspectRatio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = ResizeWithAspectRatio(img1, width=700) # Resize by width OR\n",
    "cv2.imshow('Original', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/original_image.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_result(img, figsize_, method, filename, save=False):\n",
    "    plt.figure(figsize=figsize_)\n",
    "    plt.axis('off')\n",
    "    plt.title(method)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if save:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def plot_results(result1, result2, figsize_, method, save=False):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=figsize_)\n",
    "    axs[0, 0].set_title('Before ' + method)\n",
    "    axs[0, 0].axis('off')\n",
    "    axs[0, 0].imshow(result1, cmap='gray')\n",
    "    axs[0, 1].set_title('After ' + method)\n",
    "    axs[0, 1].imshow(result2, cmap='gray')\n",
    "    if save:\n",
    "        plt.savefig(method + '_results.png')\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = ResizeWithAspectRatio(img3, width=700) # Resize by width OR\n",
    "cv2.imshow('Test', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]], np.float32)\n",
    "    sharpened_image = cv2.filter2D(img, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "def laplacian(img):\n",
    "    kernel = np.array([[-1, -1, -1],\n",
    "                       [-1, 9, -1],\n",
    "                       [-1, -1, -1]], np.float32) \n",
    "    sharpened_image = cv2.filter2D(img, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "def same(img):\n",
    "    kernel = np.array([[0, 0, 0],\n",
    "                       [0, 1, 0],\n",
    "                       [0, 0, 0]], np.float32) \n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened_image = sharpen(img1)\n",
    "resize = ResizeWithAspectRatio(sharpened_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Sharpened', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/sharpened_image.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_image = laplacian(img1)\n",
    "resize = ResizeWithAspectRatio(laplacian_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Laplacian', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/laplacian_image.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same = same(img1)\n",
    "resize = ResizeWithAspectRatio(same, width=700) # Resize by width OR\n",
    "cv2.imshow('Same', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/same_image.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = cv2.GaussianBlur(img1, (0, 0), 2.0)\n",
    "unsharp_image = cv2.addWeighted(img1, 2.5, gaussian, -1.5, 0)\n",
    "\n",
    "resize = ResizeWithAspectRatio(unsharp_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Unsharp Image', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/unsharp_image.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting (as a percentage of the picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftUp(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = 0, -(height*percent/100)\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftDown(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = 0, (height*percent/100)\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftLeft(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = -(height*percent/100), -0\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftRight(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = (height*percent/100), 0\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_image1 = shiftUp(img1, 7)\n",
    "shifted_image1 = shiftDown(shifted_image1, 88)\n",
    "shifted_image1 = shiftRight(shifted_image1, 34)\n",
    "shifted_image1 = shiftLeft(shifted_image1, 55)\n",
    "resize = ResizeWithAspectRatio(shifted_image1, width=700) # Resize by width OR\n",
    "cv2.imshow('Shifted', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/shifted_image1.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_image2 = shiftUp(img1, 6)\n",
    "shifted_image2 = shiftDown(shifted_image2, 88)\n",
    "shifted_image2 = shiftRight(shifted_image2, 5)\n",
    "shifted_image2 = shiftLeft(shifted_image2, 45)\n",
    "resize = ResizeWithAspectRatio(shifted_image2, width=700) # Resize by width OR\n",
    "cv2.imshow('Shifted', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/shifted_image2.png', result)    # mention the path you want to save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, height_percent, width_percent):\n",
    "    h1, h2 = height_percent\n",
    "    w1, w2 = width_percent\n",
    "    height, width = img.shape[:2]\n",
    "    cropped_image = img[int(height*h1/100):int(height*h2/100), int(width*w1/100):int(width*w2/100)]\n",
    "    return cropped_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = crop_image(img3, (0,50), (0,35))\n",
    "resize = ResizeWithAspectRatio(cropped, width=700) # Resize by width OR\n",
    "cv2.imshow('Cropped', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Process Data into Anki Card Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data comes in in json format which is obtained from running Google Document AI\n",
    "def format_anki(data):\n",
    "    cards = []\n",
    "    kana, kanji, jap_sen, eng_trans, eng_ex_sen = deque(), deque(), deque(), deque(), deque()\n",
    "    for obj in data:\n",
    "        #Add the text and relevant metadata to respective arrays\n",
    "        match obj[\"type\"]:\n",
    "            case \"English\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                    \"bounding\": obj[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"]\n",
    "                }\n",
    "                eng_trans.append(d)\n",
    "\n",
    "            case \"Kana\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                kana.append(d)\n",
    "\n",
    "            case \"Kanji\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                    \"bounding\": obj[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"]\n",
    "                }\n",
    "                kanji.append(d)\n",
    "\n",
    "            case \"Sentence_English\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                eng_ex_sen.append(d)\n",
    "\n",
    "            case \"Sentence_Japanese\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                jap_sen.append(d)\n",
    "                \n",
    "    # Create the fields of the anki card\n",
    "    while kana:\n",
    "        kanaAdd = kana.popleft()['text']\n",
    "        if kanji[0]['bounding'][0]['y'] < eng_trans[0]['bounding'][0]['y']:\n",
    "            kanjiAdd = kanji.popleft()['text']\n",
    "        else:\n",
    "            kanjiAdd = ''\n",
    "        japAdd = jap_sen.popleft()['text']\n",
    "        engTransAdd = eng_trans.popleft()['text']\n",
    "        engExSenAdd = eng_ex_sen.popleft()['text']\n",
    "\n",
    "        card = [\n",
    "            kanaAdd,\n",
    "            kanjiAdd,\n",
    "            japAdd,\n",
    "            engTransAdd,\n",
    "            engExSenAdd\n",
    "        ]\n",
    "        cards.append(card)\n",
    "\n",
    "    return cards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image OCR using Google Cloud Vision API (Not being used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.resources import path\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def detectText(img):\n",
    "    with io.open(img, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision_v1.types.Image(content = content)\n",
    "    response = client.text_detection(image = image)\n",
    "    texts = response.text_annotations\n",
    "    print(\"Texts:\")\n",
    "\n",
    "    df = pd.DataFrame(columns = ['locale', 'description'])\n",
    "    '''\n",
    "    for text in texts:\n",
    "        df = df._append(\n",
    "            dict(\n",
    "                local = text.locale,\n",
    "                description = text.description\n",
    "            ),\n",
    "            ignore_index = True\n",
    "        )\n",
    "    return df\n",
    "    '''\n",
    "    for text in texts:\n",
    "        print(f'\\n\"{text.description}\"')\n",
    "\n",
    "        vertices = [\n",
    "            f\"({vertex.x},{vertex.y})\" for vertex in text.bounding_poly.vertices\n",
    "        ]\n",
    "\n",
    "        print(\"bounds: {}\".format(\",\".join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "\n",
    "FILE_NAME = 'images/test_image1.jpg'\n",
    "FOLDER_PATH = r'./'\n",
    "\n",
    "print(detectText(FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image OCR using Google Document AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai  # type: ignore\n",
    "from google.cloud import documentai_v1\n",
    "\n",
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "project_id = \"japaneseocr-1\"\n",
    "location = \"us\" # Format is \"us\" or \"eu\"\n",
    "processor_id = \"eaf216b404f6b455\" # Create processor before running sample\n",
    "file_path = \"images/test_image1.jpg\"\n",
    "mime_type = \"image/jpeg\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "field_mask = \"text,entities\"  #,pages.pageNumber\"  # Optional. The fields to return in the Document object.\n",
    "# processor_version_id = \"YOUR_PROCESSOR_VERSION_ID\" # Optional. Processor version to use\n",
    "process_data = True\n",
    "\n",
    "\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    field_mask: Optional[str] = None,\n",
    "    processor_version_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    if processor_version_id:\n",
    "        # The full resource name of the processor version, e.g.:\n",
    "        # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\n",
    "        name = client.processor_version_path(\n",
    "            project_id, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        # The full resource name of the processor, e.g.:\n",
    "        # `projects/{project_id}/locations/{location}/processors/{processor_id}`\n",
    "        name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load binary data\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # For more information: https://cloud.google.com/document-ai/docs/reference/rest/v1/ProcessOptions\n",
    "    # Optional: Additional configurations for processing.\n",
    "    process_options = documentai.ProcessOptions(\n",
    "        # Process only specific pages\n",
    "        individual_page_selector=documentai.ProcessOptions.IndividualPageSelector(\n",
    "            pages=[1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name,\n",
    "        raw_document=raw_document,\n",
    "        field_mask=field_mask,\n",
    "        process_options=process_options,\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # For a full list of `Document` object attributes, reference this page:\n",
    "    # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\n",
    "    document = result.document\n",
    "\n",
    "    # Read the text recognition output from the processor\n",
    "#    print(\"The document contains the following text:\")\n",
    "#    print(document.text)\n",
    "    \n",
    "    # Write document to data.json\n",
    "    json_string = documentai_v1.Document.to_json(document)\n",
    "    dict_obj = json.loads(json_string)\n",
    "\n",
    "    if process_data:\n",
    "        filtered_data = []\n",
    "    #   filtered_data[\"text\"] = (dict_obj[\"text\"])   # Uncomment if you want the first line of the json to contain all the text\n",
    "        for e in dict_obj[\"entities\"]:\n",
    "            data = {\n",
    "                \"type\": e[\"type\"],\n",
    "                \"mentionText\": e[\"mentionText\"],\n",
    "                \"pageAnchor\": e[\"pageAnchor\"],\n",
    "                \"id\": e[\"id\"],\n",
    "            }\n",
    "            filtered_data.append(data)\n",
    "            \n",
    "        final_data = format_anki(filtered_data)\n",
    "            \n",
    "        with open(\"data.json\", mode='w') as my_file:\n",
    "            json.dump(final_data, my_file)\n",
    "    else:\n",
    "        with open(\"data.json\", mode='w') as my_file:\n",
    "            json.dump(dict_obj, my_file)\n",
    "\n",
    "process_document_sample(\n",
    "    project_id,\n",
    "    location,\n",
    "    processor_id,\n",
    "    file_path,\n",
    "    mime_type,\n",
    "    field_mask\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
