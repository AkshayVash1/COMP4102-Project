{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np # scientific computing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # reading images\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray # converting rgb images to grayscale\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'japaneseocr-1-8883b9dcab0a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img1 = rgb2gray(mpimg.imread('images/test_image.png')[:,:,:3])\n",
    "img2 = rgb2gray(mpimg.imread('images/bird.png')[:,:,:3])\n",
    "img3 = rgb2gray(mpimg.imread('images/two_words.jpg')[:,:,:3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeWithAspectRatio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "resize = ResizeWithAspectRatio(img1, width=700) # Resize by width OR\n",
    "cv2.imshow('Original', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/original_image.png', result)    # mention the path you want to save the result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "resize = ResizeWithAspectRatio(img3, width=700) # Resize by width OR\n",
    "cv2.imshow('Test', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]], np.float32)\n",
    "    sharpened_image = cv2.filter2D(img, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "def laplacian(img):\n",
    "    kernel = np.array([[-1, -1, -1],\n",
    "                       [-1, 9, -1],\n",
    "                       [-1, -1, -1]], np.float32) \n",
    "    sharpened_image = cv2.filter2D(img, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "def same(img):\n",
    "    kernel = np.array([[0, 0, 0],\n",
    "                       [0, 1, 0],\n",
    "                       [0, 0, 0]], np.float32) \n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sharpened_image = sharpen(img1)\n",
    "resize = ResizeWithAspectRatio(sharpened_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Sharpened', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/sharpened_image.png', result)    # mention the path you want to save the result\n",
    "\n",
    "\n",
    "laplacian_image = laplacian(img1)\n",
    "resize = ResizeWithAspectRatio(laplacian_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Laplacian', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/laplacian_image.png', result)    # mention the path you want to save the result\n",
    "\n",
    "\n",
    "same = same(img1)\n",
    "resize = ResizeWithAspectRatio(same, width=700) # Resize by width OR\n",
    "cv2.imshow('Same', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/same_image.png', result)    # mention the path you want to save the result\n",
    "\n",
    "\n",
    "gaussian = cv2.GaussianBlur(img1, (0, 0), 2.0)\n",
    "unsharp_image = cv2.addWeighted(img1, 2.5, gaussian, -1.5, 0)\n",
    "\n",
    "resize = ResizeWithAspectRatio(unsharp_image, width=700) # Resize by width OR\n",
    "cv2.imshow('Unsharp Image', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/unsharp_image.png', result)    # mention the path you want to save the result\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting (as a percentage of the picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftUp(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = 0, -(height*percent/100)\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftDown(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = 0, (height*percent/100)\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftLeft(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = -(height*percent/100), -0\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n",
    "\n",
    "def shiftRight(img, percent):\n",
    "    height, width = img.shape[:2]\n",
    "    tx, ty = (height*percent/100), 0\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                    [0, 1, ty]\n",
    "                                ], dtype=np.float32)\n",
    "    translated_image = cv2.warpAffine(src=img, M=translation_matrix, dsize=(width, height))\n",
    "    return translated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "shifted_image1 = shiftUp(img1, 7)\n",
    "shifted_image1 = shiftDown(shifted_image1, 88)\n",
    "shifted_image1 = shiftRight(shifted_image1, 34)\n",
    "shifted_image1 = shiftLeft(shifted_image1, 55)\n",
    "resize = ResizeWithAspectRatio(shifted_image1, width=700) # Resize by width OR\n",
    "cv2.imshow('Shifted', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/shifted_image1.png', result)    # mention the path you want to save the result\n",
    "\n",
    "\n",
    "shifted_image2 = shiftUp(img1, 6)\n",
    "shifted_image2 = shiftDown(shifted_image2, 88)\n",
    "shifted_image2 = shiftRight(shifted_image2, 5)\n",
    "shifted_image2 = shiftLeft(shifted_image2, 45)\n",
    "resize = ResizeWithAspectRatio(shifted_image2, width=700) # Resize by width OR\n",
    "cv2.imshow('Shifted', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if save:\n",
    "    result = cv2.normalize(resize, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    cv2.imwrite('images/shifted_image2.png', result)    # mention the path you want to save the result\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping (as a percentage of the picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, height_percent, width_percent):\n",
    "    h1, h2 = height_percent\n",
    "    w1, w2 = width_percent\n",
    "    height, width = img.shape[:2]\n",
    "    cropped_image = img[int(height*h1/100):int(height*h2/100), int(width*w1/100):int(width*w2/100)]\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cropped = crop_image(img3, (0,50), (0,35))\n",
    "resize = ResizeWithAspectRatio(cropped, width=700) # Resize by width OR\n",
    "cv2.imshow('Cropped', resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Process Data into Anki Card Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data comes in in json format which is obtained from running Google Document AI\n",
    "def format_anki(data):\n",
    "    cards = []\n",
    "    kana, kanji, jap_sen, eng_trans, eng_ex_sen = deque(), deque(), deque(), deque(), deque()\n",
    "    for obj in data:\n",
    "        #Add the text and relevant metadata to respective arrays\n",
    "        match obj[\"type\"]:\n",
    "            case \"English\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                    \"bounding\": obj[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"]\n",
    "                }\n",
    "                eng_trans.append(d)\n",
    "\n",
    "            case \"Kana\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                kana.append(d)\n",
    "\n",
    "            case \"Kanji\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                    \"bounding\": obj[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"]\n",
    "                }\n",
    "                kanji.append(d)\n",
    "\n",
    "            case \"Sentence_English\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                eng_ex_sen.append(d)\n",
    "\n",
    "            case \"Sentence_Japanese\":\n",
    "                d = {\n",
    "                    \"text\": obj[\"mentionText\"],\n",
    "                }\n",
    "                jap_sen.append(d)\n",
    "    print(kana)\n",
    "    print(kanji)\n",
    "    print(jap_sen)\n",
    "    print(eng_trans)\n",
    "    print(eng_ex_sen)\n",
    "\n",
    "    # Create the fields of the anki card\n",
    "    while kana:\n",
    "        kanaAdd = kana.popleft()['text']\n",
    "        if kanji[0]['bounding'][0]['y'] < eng_trans[0]['bounding'][0]['y']:\n",
    "            kanjiAdd = kanji.popleft()['text']\n",
    "        else:\n",
    "            kanjiAdd = ''\n",
    "        japAdd = jap_sen.popleft()['text']\n",
    "        engTransAdd = eng_trans.popleft()['text']\n",
    "        engExSenAdd = eng_ex_sen.popleft()['text'].replace('\\n', ' ')\n",
    "\n",
    "        card = [\n",
    "            kanaAdd,\n",
    "            kanjiAdd,\n",
    "            japAdd,\n",
    "            engTransAdd,\n",
    "            engExSenAdd\n",
    "        ]\n",
    "        cards.append(card)\n",
    "    \n",
    "    return cards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image OCR using Google Cloud Vision API (Not being used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from importlib.resources import path\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def detectText(img):\n",
    "    with io.open(img, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision_v1.types.Image(content = content)\n",
    "    response = client.text_detection(image = image)\n",
    "    texts = response.text_annotations\n",
    "    print(\"Texts:\")\n",
    "\n",
    "    df = pd.DataFrame(columns = ['locale', 'description'])\n",
    "    \n",
    "#    for text in texts:\n",
    "#        df = df._append(\n",
    "#            dict(\n",
    "#                local = text.locale,\n",
    "#                description = text.description\n",
    "#            ),\n",
    "#            ignore_index = True\n",
    "#        )\n",
    "#    return df\n",
    "    \n",
    "    for text in texts:\n",
    "        print(f'\\n\"{text.description}\"')\n",
    "\n",
    "        vertices = [\n",
    "            f\"({vertex.x},{vertex.y})\" for vertex in text.bounding_poly.vertices\n",
    "        ]\n",
    "\n",
    "        print(\"bounds: {}\".format(\",\".join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "\n",
    "FILE_NAME = 'images/test_image1.jpg'\n",
    "FOLDER_PATH = r'./'\n",
    "\n",
    "print(detectText(FILE_NAME))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image OCR using Google Document AI API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
